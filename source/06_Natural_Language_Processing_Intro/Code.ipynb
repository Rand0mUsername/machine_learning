{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n Noun <br>\n",
    "a Adjective <br>\n",
    "r Adverb <br>\n",
    "v Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Skinite nltk na odredjenu putanju i to unesite u path (linija ispod)\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.data import path\n",
    "path.append(\"/home/milutin/Documents/Projects/MachineLearning/machine-learning/data/06_NLP/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Dobro dosli na vezbe iz predmeta `Masinsko ucenje`. \n",
    "Danas radimo uvod u NLP (Natural Language Processing). Koristicemo Python biblioteku NLTK.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dobro dosli na vezbe iz predmeta `Masinsko ucenje`.',\n",
       " 'Danas radimo uvod u NLP (Natural Language Processing).',\n",
       " 'Koristicemo Python biblioteku NLTK.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dobro',\n",
       " 'dosli',\n",
       " 'na',\n",
       " 'vezbe',\n",
       " 'iz',\n",
       " 'predmeta',\n",
       " '`Masinsko',\n",
       " 'ucenje`',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wo', \"n't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"won't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['won', \"'\", 't']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(\"won't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"won't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(\"won't\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"can't\", 'do', 'this', 'I', \"won't\", 'do', 'that']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(\"I can't do this. I won't do that.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**: Trying to shorten a word with simple regex rules\n",
    "\n",
    "Word stemming means removing affixes from words and return the root word. \n",
    "Example: The stem of the word working => work.\n",
    "Search engines use this technique when indexing pages, so many people write different versions for the same word and all of them are stemmed to the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer    # least aggressive\n",
    "from nltk.stem import LancasterStemmer # most agressive\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer  # languages other than English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('dancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dancer'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('dancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster.stem('cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dant'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster.stem('dancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dant'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster.stem('dance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = RegexpStemmer('ing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp.stem('dancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp.stem('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**: Trying to find the root word with linguistics rules (with the use of regexes)\n",
    "\n",
    "Note: Lemmatization won't really work on single words alone without context or knowledge of its POS (Parts of Speech) tag (i.e. we need to know whether the word is a noun, verb, adjective, adverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dance'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dancing', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('dancing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'hello world, hello people, this is hello world example of word count.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tok = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world',\n",
       " ',',\n",
       " 'hello',\n",
       " 'people',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'hello',\n",
       " 'world',\n",
       " 'example',\n",
       " 'of',\n",
       " 'word',\n",
       " 'count',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd33f629a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of stopwords for english\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "# Split paragraph into words\n",
    "words = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " '.',\n",
       " 'All',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_filtered = [w for w in words if w not in stopword_list]\n",
    "words_filtered\n",
    "# wordsFiltered = []\n",
    "# for w in words:\n",
    "#     if w not in stopWords:\n",
    "#         wordsFiltered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_punctuation_list = set(stopword_list).union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " 'All',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_filtered = [w for w in words if w not in stopwords_punctuation_list]\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " 'all',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_filtered = [w.lower() for w in words if w not in stopwords_punctuation_list]\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas, Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonym set - synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sArr = wordnet.synsets('win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('win.n.01'),\n",
       " Synset('winnings.n.01'),\n",
       " Synset('win.v.01'),\n",
       " Synset('acquire.v.05'),\n",
       " Synset('gain.v.05'),\n",
       " Synset('succeed.v.01')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a victory (as in a race or other competition)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sArr[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['win',\n",
       " 'winnings',\n",
       " 'win',\n",
       " 'profits',\n",
       " 'win',\n",
       " 'acquire',\n",
       " 'win',\n",
       " 'gain',\n",
       " 'gain',\n",
       " 'advance',\n",
       " 'win',\n",
       " 'pull_ahead',\n",
       " 'make_headway',\n",
       " 'get_ahead',\n",
       " 'gain_ground',\n",
       " 'succeed',\n",
       " 'win',\n",
       " 'come_through',\n",
       " 'bring_home_the_bacon',\n",
       " 'deliver_the_goods']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synArr = []\n",
    "for syn in sArr:\n",
    "    for lem in syn.lemmas():\n",
    "        synArr.append(lem.name())\n",
    "synArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acquire',\n",
       " 'advance',\n",
       " 'bring_home_the_bacon',\n",
       " 'come_through',\n",
       " 'deliver_the_goods',\n",
       " 'gain',\n",
       " 'gain_ground',\n",
       " 'get_ahead',\n",
       " 'make_headway',\n",
       " 'profits',\n",
       " 'pull_ahead',\n",
       " 'succeed',\n",
       " 'win',\n",
       " 'winnings'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(synArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('win.v.01')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lose'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi = sArr[2]\n",
    "print(woi) # Treba nam v\n",
    "woi.lemmas()[0].antonyms()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['losings', 'lose', 'lose', 'fall_back', 'fail']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antArr = []\n",
    "for syn in sArr:\n",
    "    for lem in syn.lemmas():\n",
    "        for ant in lem.antonyms():\n",
    "            antArr.append(ant.name())\n",
    "antArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypernyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synsets are organized in a kind of inheritance tree. More abstract terms are known as <b>hypernyms</b> and more specific terms are <b> hyponyms</b>. This tree can be traced all the way up to a root hypernym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('victory.n.01')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sArr[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('checkmate.n.01'),\n",
       " Synset('fall.n.10'),\n",
       " Synset('independence.n.02'),\n",
       " Synset('landslide.n.01'),\n",
       " Synset('last_laugh.n.01'),\n",
       " Synset('pyrrhic_victory.n.01'),\n",
       " Synset('runaway.n.01'),\n",
       " Synset('service_break.n.01'),\n",
       " Synset('slam.n.01'),\n",
       " Synset('walk-in.n.03'),\n",
       " Synset('win.n.01')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sArr[0].hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sArr[0].root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams, Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "# https://en.wikipedia.org/wiki/Bigram\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize('hello world, hello people, this is hello world example of word count.')\n",
    "stopwords_punctuation_list = set(stopwords.words('english')).union(set(punctuation))\n",
    "words_clean = [w.lower() for w in words if w not in stopwords_punctuation_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world',\n",
       " 'hello',\n",
       " 'people',\n",
       " 'hello',\n",
       " 'world',\n",
       " 'example',\n",
       " 'word',\n",
       " 'count']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('example', 'word'), ('word', 'count')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(words_clean)\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('example', 'word', 'count'), ('world', 'example', 'word')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = TrigramCollocationFinder.from_words(words_clean)\n",
    "finder.nbest(TrigramAssocMeasures.likelihood_ratio, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(text, n=3):\n",
    "    return [text[i:i + n] for i in range(len(text) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hel', 'ell', 'llo', 'lo ', 'o w', ' wo', 'wor', 'orl', 'rld', 'ld,', 'd, ', ', h', ' he', 'hel', 'ell', 'llo', 'lo ', 'o p', ' pe', 'peo', 'eop', 'opl', 'ple', 'le,', 'e, ', ', t', ' th', 'thi', 'his', 'is ', 's i', ' is', 'is ', 's h', ' he', 'hel', 'ell', 'llo', 'lo ', 'o w', ' wo', 'wor', 'orl', 'rld', 'ld ', 'd e', ' ex', 'exa', 'xam', 'amp', 'mpl', 'ple', 'le ', 'e o', ' of', 'of ', 'f w', ' wo', 'wor', 'ord', 'rd ', 'd c', ' co', 'cou', 'oun', 'unt', 'nt.']\n"
     ]
    }
   ],
   "source": [
    "print(ngrams('hello world, hello people, this is hello world example of word count.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize('And now for something completely different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Space Model** is conceptualizing language as a whole lot of numbers\n",
    "\n",
    "**Bag-of-Words (BoW)**: Counting each document/sentence as a vector of numbers, with each number representing the count of a word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'John likes to watch movies. Mary likes movies too. \\\n",
    "John also likes to watch football games.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'movies',\n",
       " '.',\n",
       " 'Mary',\n",
       " 'likes',\n",
       " 'movies',\n",
       " 'too',\n",
       " '.',\n",
       " 'John',\n",
       " 'also',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'football',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_punct_list = set(stopwords.words('english')).union(set(punctuation))\n",
    "words_clean = [w.lower() for w in words if w not in stopwords_punctuation_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'likes',\n",
       " 'watch',\n",
       " 'movies',\n",
       " 'mary',\n",
       " 'likes',\n",
       " 'movies',\n",
       " 'john',\n",
       " 'also',\n",
       " 'likes',\n",
       " 'watch',\n",
       " 'football',\n",
       " 'games']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(words_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'also': 1,\n",
       "          'football': 1,\n",
       "          'games': 1,\n",
       "          'john': 2,\n",
       "          'likes': 3,\n",
       "          'mary': 1,\n",
       "          'movies': 2,\n",
       "          'watch': 2})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('john', 'likes')\n",
      "('likes', 'watch')\n",
      "('watch', 'movies')\n",
      "('movies', 'mary')\n",
      "('mary', 'likes')\n",
      "('likes', 'movies')\n",
      "('movies', 'john')\n",
      "('john', 'also')\n",
      "('also', 'likes')\n",
      "('likes', 'watch')\n",
      "('watch', 'football')\n",
      "('football', 'games')\n"
     ]
    }
   ],
   "source": [
    "for gram in ngrams(words_clean, n):\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String diffrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# String edit distance (Levenshtein)\n",
    "edit_distance(\"rain\", \"shine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rain\n",
    "# **  *\n",
    "# shine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
