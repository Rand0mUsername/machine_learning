{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today:\n",
    "* Neural Networks\n",
    "    * Neuron \n",
    "    * Arhitecture\n",
    "    * Activation functions\n",
    "    * Cost function\n",
    "    * Feed-forward\n",
    "    * Backpropagation\n",
    "  \n",
    "### Resources:\n",
    "* Neural Networks: http://neuralnetworksanddeeplearning.com/\n",
    "* Neural Networks: http://karpathy.github.io/neuralnets/\n",
    "* Neural Networks: http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "\n",
    "https://www.jessicayung.com/explaining-tensorflow-code-for-a-multilayer-perceptron/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/05_kNN/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/05_kNN/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/05_kNN/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/05_kNN/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets(\"../../data/05_kNN/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f06d7385f98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06d93af6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = mnist.train.images[10].reshape((28, 28))\n",
    "plt.imshow(image, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 784        # MNIST data input (img shape: 28*28)\n",
    "n_labels = 10           # MNIST total classes (0-9 digits)\n",
    "learning_rate = 0.5\n",
    "iterations = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph input\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "y = tf.placeholder(tf.float32, [None, n_labels])\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(tf.random_normal([n_features, n_labels]))\n",
    "b = tf.Variable(tf.random_normal([n_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "hypothesis = tf.nn.softmax(tf.add(tf.matmul(X, W), b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypothesis), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1143\n",
      "100 0.7447\n",
      "200 0.8059\n",
      "300 0.8357\n",
      "400 0.8459\n",
      "500 0.8549\n",
      "600 0.8637\n",
      "700 0.8711\n",
      "800 0.8701\n",
      "900 0.8712\n",
      "1000 0.8768\n",
      "1100 0.881\n",
      "1200 0.8875\n",
      "1300 0.8842\n",
      "1400 0.8871\n",
      "1500 0.89\n",
      "1600 0.8912\n",
      "1700 0.8883\n",
      "1800 0.8935\n",
      "1900 0.8927\n",
      "2000 0.8926\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for step in range(iterations):\n",
    "        \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        \n",
    "        sess.run(train_step, feed_dict={X: batch_xs, y: batch_ys})\n",
    "        \n",
    "        if (step % 100) == 0:\n",
    "            print(step, sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))\n",
    "    \n",
    "    print(iterations, sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-59-079e81acbe9f>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch: 0001 cost=341.882086286\n",
      "Epoch: 0002 cost=99.498621944\n",
      "Epoch: 0003 cost=73.752815736\n",
      "Epoch: 0004 cost=59.628556610\n",
      "Epoch: 0005 cost=49.453289975\n",
      "Epoch: 0006 cost=43.802042105\n",
      "Epoch: 0007 cost=38.559764022\n",
      "Epoch: 0008 cost=34.596790018\n",
      "Epoch: 0009 cost=31.885243859\n",
      "Epoch: 0010 cost=29.095908964\n",
      "Epoch: 0011 cost=26.805818067\n",
      "Epoch: 0012 cost=25.005075455\n",
      "Epoch: 0013 cost=23.765815960\n",
      "Epoch: 0014 cost=22.753009919\n",
      "Epoch: 0015 cost=21.539625950\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8858\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "                                                            Y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
